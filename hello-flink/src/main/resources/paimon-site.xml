<configuration>

    <!-- 通用配置：start -->
    <property>
        <name>bucket</name>
        <value>3</value>
        <description>Bucket number for file store.</description>
        <default>1</default>
    </property>

    <property>
        <name>bucket-key</name>
        <value>waybill_code</value>
        <description>
            Specify the paimon distribution policy. Data is assigned to each bucket according to the hash value of bucket-key.
            If you specify multiple fields, delimiter is ','.
            If not specified, the primary key will be used; if there is no primary key, the full row will be used.
        </description>
        <default>(none)</default>
    </property>

    <property>
        <name>manifest.format</name>
        <value>orc</value>
        <description>
            Specify the message format of manifest files.

            Possible values:
            "orc": ORC file format.
            "parquet": Parquet file format.
            "avro": Avro file format.
        </description>
        <default>avro</default>
    </property>

    <property>
        <name>file.format</name>
        <value>orc</value>
        <description>
            Specify the message format of data files, currently orc, parquet and avro are supported.
            Possible values:
            "orc": ORC file format.
            "parquet": Parquet file format.
            "avro": Avro file format.
        </description>
        <default>orc</default>
    </property>

    <!-- 每个lsm-tree的层数（包含第0层），如果设置为5，那么lsm-tree的层分别为0,1,2,3,4。如果不配置该值，那么lsm-tree的层数默认为num-sorted-run.compaction-trigger+1 -->
    <property>
        <name>num-levels</name>
        <value>(none)</value>
        <description>Total level number, for example, there are 3 levels, including 0,1,2 levels.</description>
        <default>(none)</default>
    </property>
    <!-- 通用配置：end -->

    <!-- query配置：start -->
    <!-- 仅在流读append only表时有效，按partition的顺序来生成split。最终的效果是按分区顺序流读数据文件。 -->
    <property>
        <name>scan.plan-sort-partition</name>
        <value>true</value>
        <description>
            Whether to sort plan files by partition fields, this allows you to read according to the partition order, even if your partition writes are out of order.
            It is recommended that you use this for streaming read of the 'append-only' table. By default, streaming read will read the full snapshot first. In order to avoid the disorder reading for partitions, you can open this option.
        </description>
        <default>false</default>
    </property>

    <property>
        <name>scan.infer-parallelism</name>
        <value>true</value>
        <description>If it is false, parallelism of source are set by global parallelism. Otherwise, source parallelism is inferred from splits number (batch mode) or bucket number(streaming mode).</description>
        <default>true</default>
    </property>

    <property>
        <name>scan.push-down</name>
        <value>true</value>
        <description>
            If true, flink will push down projection, filters, limit to the source. The cost is that it is difficult to reuse the source in a job. With flink 1.18 or higher version, it is possible to reuse the source even with projection push down.
        </description>
        <default>true</default>
    </property>

    <property>
        <name>scan.mode</name>
        <value>compacted-full</value>
        <description>
            Specify the scanning behavior of the source.

            Possible values:
            "default": Determines actual startup mode according to other table properties. If "scan.timestamp-millis" is set the actual startup mode will be "from-timestamp", and if "scan.snapshot-id" or "scan.tag-name" is set the actual startup mode will be "from-snapshot". Otherwise the actual startup mode will be "latest-full".
            "latest-full": For streaming sources, produces the latest snapshot on the table upon first startup, and continue to read the latest changes. For batch sources, just produce the latest snapshot but does not read new changes.
            "full": Deprecated. Same as "latest-full".
            "latest": For streaming sources, continuously reads latest changes without producing a snapshot at the beginning. For batch sources, behaves the same as the "latest-full" startup mode.
            "compacted-full": For streaming sources, produces a snapshot after the latest compaction on the table upon first startup, and continue to read the latest changes. For batch sources, just produce a snapshot after the latest compaction but does not read new changes. Snapshots of full compaction are picked when scheduled full-compaction is enabled.
            "from-timestamp": For streaming sources, continuously reads changes starting from timestamp specified by "scan.timestamp-millis", without producing a snapshot at the beginning. For batch sources, produces a snapshot at timestamp specified by "scan.timestamp-millis" but does not read new changes.
            "from-snapshot": For streaming sources, continuously reads changes starting from snapshot specified by "scan.snapshot-id", without producing a snapshot at the beginning. For batch sources, produces a snapshot specified by "scan.snapshot-id" or "scan.tag-name" but does not read new changes.
            "from-snapshot-full": For streaming sources, produces from snapshot specified by "scan.snapshot-id" on the table upon first startup, and continuously reads changes. For batch sources, produces a snapshot specified by "scan.snapshot-id" but does not read new changes.
            "incremental": Read incremental changes between start and end snapshot or timestamp.
        </description>
        <default>default</default>
    </property>

    <property>
        <name>scan.snapshot-id</name>
        <value>5</value>
        <description>Optional snapshot id used in case of "from-snapshot" or "from-snapshot-full" scan mode</description>
        <default>(none)	</default>
    </property>

    <property>
        <name>incremental-between</name>
        <value>20,25</value>
        <description>Read incremental changes between start snapshot (exclusive) and end snapshot, for example, '5,10' means changes between snapshot 5 and snapshot 10.</description>
        <default>(none)</default>
    </property>

    <property>
        <name>incremental-between-timestamp</name>
        <value>1704263750108,1704274182534</value>
        <description>Read incremental changes between start timestamp (exclusive) and end timestamp, for example, 't1,t2' means changes between timestamp t1 and timestamp t2.</description>
        <default>(none)</default>
    </property>

    <property>
        <name>incremental-between-scan-mode</name>
        <value>delta</value>
        <description>
            Scan kind when Read incremental changes between start snapshot (exclusive) and end snapshot, 'delta' for scan newly changed files between snapshots, 'changelog' scan changelog files between snapshots.

            Possible values:
                "delta": Scan newly changed files between snapshots.
                "changelog": Scan changelog files between snapshots.
        </description>
        <default>delta</default>
    </property>

    <property>
        <name>scan.timestamp-millis</name>
        <value>1697179963000</value>
        <description>Optional timestamp used in case of "from-timestamp" scan mode.</description>
        <default>(none)</default>
    </property>

    <!-- 在流读中，隔多长时间来发现新的snapshot -->
    <property>
        <name>continuous.discovery-interval</name>
        <value>1 min</value>
        <description>The discovery interval of continuous reading.</description>
        <default>10 s</default>
    </property>

    <!-- 流读时是否读取OVERWRITE类型的snapshot。如果为true，则会读取overwrite类型的snapshot，读取出被删除的data file中的RowData，这些RowData的类型都是-D，代表是被删除的数据。 -->
    <property>
        <name>streaming-read-overwrite</name>
        <value>false</value>
        <description>Whether to read the changes from overwrite in streaming mode.</description>
        <default>false</default>
    </property>

    <!-- In the case of 'changelog-producer' = 'lookup', by default, the lookup will be completed at checkpointing, which will block the checkpoint. If you want an asynchronous lookup, you can use 'changelog-producer.lookup-wait' = 'false'. -->
    <property>
        <name>changelog-producer.lookup-wait</name>
        <value>true</value>
        <description>
            When changelog-producer is set to LOOKUP, commit will wait for changelog generation by lookup.
        </description>
        <default>true</default>
    </property>
    <!-- query配置：end -->

    <!-- write配置：start -->
    <!-- 控制写入的data file的大小，以及compaction形成的文件的大小，超出该配置则会生成一个新的data file -->
    <property>
        <name>target-file-size</name>
        <value>1 gb</value>
        <description>Target size of a file.</description>
        <default>128 mb</default>
    </property>

    <!-- 控制writer的buffer的总大小。writer在写入时，每一个bucket拥有一个buffer，用于作lsm-tree的buffer。但所有bucket的buffer都共用writer buffer的内存池。也就是说，即使bucket再增加，writer中用于缓冲数据的内存总大小也不会增加。 -->
    <!-- Writer’s memory buffer, shared and preempted by all writers of a single task. This memory value can be adjusted by the write-buffer-size table property. -->
    <property>
        <name>write-buffer-size</name>
        <value>1 gb</value>
        <description>Amount of data to build up in memory before converting to a sorted on-disk file.</description>
        <default>256 mb</default>
    </property>

    <!-- 控制writer的写入模型，也就是该table是primary key table还是append only table。一般不用配置，paimon通过table定义中是否有主键，来决定该table的类型。 -->
    <property>
        <name>write-mode</name>
        <value>auto</value>
        <description>
            Specify the write mode for table.
            Possible values:
            "auto": The change-log for table with primary key, append-only for table without primary key.
            "append-only": The table can only accept append-only insert operations. Neither data deduplication nor any primary key constraints will be done when inserting rows into paimon.
            "change-log": The table can accept insert/delete/update operations.
        </description>
        <default>auto</default>
    </property>

    <!-- 是否写入任务中将compaction内容去除，一般在writer中，会定期触发compaction，让CompactManager来进行文件合并。也就是在writer任务中，既负责数据写入，也负责文件合并。
      开启该配置后，任务则只负责数据写入，不再负责文件合并，也就是writer不再触发compaction。而是提交一个专门的文件合并任务(paimon action jar包)，来进行文件合并。从而减少写入任务的负担。 -->
    <property>
        <name>write-only</name>
        <value>false</value>
        <description>If set to true, compactions and snapshot expiration will be skipped. This option is used along with dedicated compact jobs.</description>
        <default>false</default>
    </property>

    <!--
        该参数用于控制在overwrite时，是否使用动态分区来overwrite。当该参数为true时，会根据insert的value中的分区字段的值，来决定要overwrite哪个分区。
        insert overwrite paimon_waybill_c_overwrite values('1','hello world','2023-10-25',2),('2','tt','2023-10-26',2);

        如果该参数为false，则根据insert语句中，partition( dt='2023-10-25')中给出的静态分区来决定要overwrite哪个分区，而不是再动态决定了。
        insert overwrite paimon_waybill_c_overwrite /*+ OPTIONS('dynamic-partition-overwrite'='false') */  partition( dt='2023-10-25') select id,name,ts from paimon_waybill_c_overwrite where false;
     -->
    <property>
        <name>dynamic-partition-overwrite</name>
        <value>false</value>
        <description>Whether only overwrite dynamic partition when overwriting a partitioned table with dynamic partition columns. Works only when the table has partition keys.</description>
        <default>true</default>
    </property>


    <!-- 写data file文件时，攒一批数据(在内存中)一起写入的数量，减少该配置可以减少写入orc文件时使用的内存量。 -->
    <property>
        <name>orc.write.batch-size</name>
        <value>1024</value>
        <description>
            write batch size for orc.
            The memory consumed by writing columnar (ORC, Parquet, etc.) file. Decreasing the orc.write.batch-size option can reduce the consumption of memory for ORC format.
        </description>
        <default>1024</default>
    </property>

    <!-- 仅供append-only表使用，AppendOnlyWriter在将数据写入文件前，是否要先将数据存储到内存缓冲区。 -->
    <property>
        <name>write-buffer-for-append</name>
        <value>true</value>
        <description>This option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.</description>
        <default>false</default>
    </property>

    <!-- primary key表和append-only表都使用该配置，用来控制当写入buffer满了以后，是否需要将buffer中的数据spill到本地磁盘。配置该属性后，buffer的put方法永远返回true
         ，避免了在checkpoint期间，由于buffer满了而flush，导致一次checkpoint会产生多个sorted run的问题，从而减少数据文件的数量。 -->
    <property>
        <name>write-buffer-spillable</name>
        <value>true</value>
        <description>This option only works for append-only table. Whether the write use write buffer to avoid out-of-memory error.</description>
        <default>(none)</default>
    </property>
    <!-- write配置：end -->

    <!-- compaction配置：start   -->
    <!-- 不同的选项对应使用不同的org.apache.paimon.mergetree.compact.MergeFunction的实现类 -->
    <property>
        <name>merge-engine</name>
        <value>partial-update</value>
        <description>
            Specify the merge engine for table with primary key.
            Possible values:
                "deduplicate": De-duplicate and keep the last row. 对应org.apache.paimon.mergetree.compact.DeduplicateMergeFunction
                "partial-update": Partial update non-null fields.  对应org.apache.paimon.mergetree.compact.PartialUpdateMergeFunction
                "aggregation": Aggregate fields with same primary key.  对应org.apache.paimon.mergetree.compact.AggregateMergeFunction
                "first-row": De-duplicate and keep the first row.  对应org.apache.paimon.mergetree.compact.FirstRowMergeFunction
        </description>
        <default>deduplicate</default>
    </property>

    <!-- 在进行数据合并时，如果数据的RowKind是回撤的(DELETE、UPDATE_BEFORE)，是否需要忽略。如果不忽略，则会报错。partual-update不支持回撤数据 -->
    <property>
        <name>partial-update.ignore-delete</name>
        <value>false</value>
        <description>Whether to ignore delete records in partial-update mode.</description>
        <default>false</default>
    </property>

    <!-- 在进行数据合并时，如果数据的RowKind是回撤的(DELETE、UPDATE_BEFORE)，是否需要忽略。如果不忽略也不会报错，可以使用回撤数据作为去重后的最后一条数据 -->
    <property>
        <name>deduplicate.ignore-delete</name>
        <value>false</value>
        <description>Whether to ignore delete records in deduplicate mode.</description>
        <default>false</default>
    </property>

    <!-- 在进行数据合并时，如果数据的RowKind是回撤的(DELETE、UPDATE_BEFORE)，是否需要忽略。如果不忽略，则会报错。first-row不支持回撤数据 -->
    <property>
        <name>first-row.ignore-delete</name>
        <value>false</value>
        <description>Whether to ignore delete records in first-row mode.</description>
        <default>false</default>
    </property>

    <property>
        <name>changelog-producer</name>
        <value>full-compaction</value>
        <description>
            Whether to double write to a changelog file. This changelog file keeps the details of data changes, it can be read directly during stream reads. This can be applied to tables with primary keys.

            Possible values:
            "none": No changelog file.
            "input": Double write to a changelog file when flushing memory table, the changelog is from input.
            "full-compaction": Generate changelog files with each full compaction.
            "lookup": Generate changelog files through 'lookup' before committing the data writing.
        </description>
        <default>none</default>
    </property>

    <property>
        <name>full-compaction.delta-commits</name>
        <value>5</value>
        <description>Full compaction will be constantly triggered after delta commits.</description>
        <default>(none)</default>
    </property>

    <!-- 在进行compaction时，需要把lsm-tree中多个文件合并成一个完全有序的文件，才能按key使用merge-engine进行合并。该配置就是用来控制如何将多个key有序的文件合成一个key完全有序的文件。 -->
    <property>
        <name>sort-engine</name>
        <value>loser-tree</value>
        <description>
            Specify the sort engine for table with primary key.

            Possible values:
                "min-heap": Use min-heap for multiway sorting.
                "loser-tree": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.
        </description>
        <default>loser-tree</default>
    </property>

    <property>
        <name>sequence.field</name>
        <value>ts</value>
        <description>
            The field that generates the sequence number for primary key table, the sequence number determines which data is the most recent.
        </description>
        <default>(none)</default>
    </property>

    <!-- 下面两个配置仅针对append-only表有效 -->
    <!-- 在append only表中使用，用于控制一个bucket中至少有几个data file，当这些data file的文件大小超过阈值(target-file-size)时，才会触发compaction。 -->
    <!-- 这个配置的目的是减少compaction的触发，因为假如一个bucket中每个新进入的data file的大小都超过了阈值，那么每新进入一个data file，就会触发一次compaction。有了该配置，进入bucket的data file的size即使超过了阈值，也不会马上进行compaction，而是等compaction.min.file-num凑齐 -->
    <property>
        <name>compaction.min.file-num</name>
        <value>3</value>
        <description>
            For file set [f_0,...,f_N], the minimum file number which satisfies sum(size(f_i)) >= targetFileSize to trigger a compaction for append-only table.
            This value avoids almost-full-file to be compacted, which is not cost-effective.
        </description>
        <default>5</default>
    </property>

    <!-- 在append only表中使用，用于控制一个bucket中最多有几个data file，即使这些文件的大小加起来都没有超过阈值(target-file-size)，也会触发compaction。 -->
    <!-- 这个配置的目的是减少小文件数量，因为假设bucket中新进入的data file的size都很小，那么bucket中的data file会越来越多，而且这些文件加起来的大小也凑不到阈值。有了该配置，当bucket中的data file的大小即使一直很小，但是当数量超过了compaction.max.file-num，即使这些文件的大小加起来没有超过阈值，也会触发compaction。 -->
    <property>
        <name>compaction.max.file-num</name>
        <value>50</value>
        <description>
            For file set [f_0,...,f_N], the maximum file number to trigger a compaction for append-only table, even if sum(size(f_i)) less than targetFileSize.
            This value avoids pending too much small files, which slows down the performance.
        </description>
        <default>50</default>
    </property>

    <!-- 下面三个配置，控制在触发compaction时，从bucket的lsm-tree中挑选的文件是否满足阈值。如果满足，则提交compaction task，如果不满足，则忽略此次compaction触发。 -->
    <property>
        <name>compaction.max-size-amplification-percent</name>
        <value>200</value>
        <description>
            The size amplification is defined as the amount (in percentage) of additional storage needed to store a single byte of data in the merge tree for changelog mode table.
        </description>
        <default>200</default>
    </property>

    <property>
        <name>compaction.size-ratio</name>
        <value>1</value>
        <description>
            Percentage flexibility while comparing sorted run size for changelog mode table. If the candidate sorted run(s) size is 1% smaller than the next sorted run's size, then include next sorted run into this candidate set.
        </description>
        <default>1</default>
    </property>

    <!-- 该配置用来控制，当lsm-tree中有几个sorted run时，才需要进行compaction。需要sorted run的数量比该配置大，才可以进行compaction，相等也不行。runs.size() > numRunCompactionTrigger -->
    <!-- Paimon uses LSM tree which supports a large number of updates. LSM organizes files in several sorted runs. When querying records from an LSM tree, all sorted runs must be combined to produce a complete view of all records. -->
    <!-- One can easily see that too many sorted runs will result in poor query performance. To keep the number of sorted runs in a reasonable range, Paimon writers will automatically perform compactions. The following table property determines the minimum number of sorted runs to trigger a compaction. -->
    <!-- Compaction will become less frequent when num-sorted-run.compaction-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. This is a trade-off between writing and query performance. -->
    <property>
        <name>num-sorted-run.compaction-trigger</name>
        <value>5</value>
        <description>The sorted run number to trigger compaction. Includes level0 files (one file one sorted run) and high-level runs (one level one sorted run).</description>
        <default>5</default>
    </property>

    <!-- 当checkpoint完成时，writer需要将write buffer中的剩余内容flush到磁盘，形成一个新的sorted run。flush完成后，会触发compaction，在bucket的lsm-tree中进行文件挑选。如果满足阈值，则会提交compaction task。
            该配置如果为true，writer就会处于阻塞中，等待compaction task执行完成后，再继续后续处理，也就是新的数据的写入。 -->
    <property>
        <name>commit.force-compact</name>
        <value>false</value>
        <description>Whether to force a compaction before commit.</description>
        <default>false</default>
        <Required/>
    </property>

    <!-- 该控制用来判断，writer在触发compaction前，是否要等待上一次提交的compaction task执行完成。以及writer在checkpoint完成时，提交了compaction task后，是否需要等待compaction task执行完成，再继续进行后续处理。 -->
    <!-- When the number of sorted runs is small, Paimon writers will perform compaction asynchronously in separated threads, so records can be continuously written into the table. However, to avoid unbounded growth of sorted runs, writers will pause writing when the number of sorted runs hits the threshold. The following table property determines the threshold. -->
    <!-- Write stalls will become less frequent when num-sorted-run.stop-trigger becomes larger, thus improving writing performance. However, if this value becomes too large, more memory and CPU time will be needed when querying the table. If you are concerned about the OOM problem, please configure the following option. Its value depends on your memory size. -->
    <!-- Compaction is inherently asynchronous, but if you want it to be completely asynchronous and not blocking writing, expect a mode to have maximum writing throughput, the compaction can be done slowly and not in a hurry. You can use the following strategies for your table:
        num-sorted-run.stop-trigger = 2147483647
        sort-spill-threshold = 10
        This configuration will generate more files during peak write periods and gradually merge into optimal read performance during low write periods.
    -->
    <property>
        <name>num-sorted-run.stop-trigger</name>
        <value>10</value>
        <description>The number of sorted runs that trigger the stopping of writes, the default value is 'num-sorted-run.compaction-trigger' + 1.</description>
        <default>(none)</default>
    </property>

    <!-- 在写任务中，当需要compaction时，需要读取data file（一般为orc格式）。该配置控制从data file读取数据时，一批次读取多少行数据。减少该配置可以减少读取data file所使用的内存量。
        默认值1024基本是最优的，可以不用动。This number is carefully chosen to minimize overhead and typically allows one VectorizedRowBatch to fit in cache.-->
    <property>
        <name>read.batch-size</name>
        <value>1024</value>
        <description>
            Read batch size for orc and parquet.
            If the row is very large, reading too many lines of data at once will consume a lot of memory when making a compaction. Reducing the read.batch-size option can alleviate the impact of this case.
        </description>
        <default>1024</default>
    </property>

    <!-- 针对主键表的配置，设置如何对多个sorted file中的文件进行归并排序，形成按key进行全排序的数据。只有获得这样的数据，才能够使用merge-engine对相同key的数据进行合并。默认使用loser-tree进行排序，性能更好。 -->
    <property>
        <name>sort-engine</name>
        <value>loser-tree</value>
        <description>
            Specify the sort engine for table with primary key.
            Possible values:
                "min-heap": Use min-heap for multiway sorting.
                "loser-tree": Use loser-tree for multiway sorting. Compared with heapsort, loser-tree has fewer comparisons and is more efficient.
        </description>
        <default>loser-tree</default>
    </property>
    <!-- compaction配置：end -->

    <!-- snapshot过期配置:start  -->
    <property>
        <name>snapshot.num-retained.max</name>
        <value>10</value>
        <description>The maximum number of completed snapshots to retain. Should be greater than or equal to the minimum number.</description>
        <default>2147483647</default>
        <Required/>
    </property>

    <property>
        <name>snapshot.num-retained.min</name>
        <value>3</value>
        <description>The minimum number of completed snapshots to retain. Should be greater than or equal to 1.</description>
        <default>10</default>
        <Required/>
    </property>

    <property>
        <name>snapshot.time-retained</name>
        <value>5 min</value>
        <description>The maximum time of completed snapshots to retain.</description>
        <default>1 h</default>
        <Required/>
    </property>
    <!-- snapshot过期配置:end  -->

    <!-- partition过期配置:start  -->
    <!-- 分区过期时间，会用当前时间减去该值，看是否大于遍历的分区的时间，如果是的话，则删除该分区 -->
    <property>
        <name>partition.expiration-time</name>
        <value>1 d</value>
        <description>The expiration interval of a partition. A partition will be expired if it‘s lifetime is over this value. Partition time is extracted from the partition value.</description>
        <default>(none)</default>
    </property>

    <!-- 分区过期的检查间隔，隔该配置的时间，才会再进行一次分区过期处理 -->
    <property>
        <name>partition.expiration-check-interval</name>
        <value>1 min</value>
        <description>The check interval of partition expiration.</description>
        <default>(none)</default>
    </property>

    <!-- 分区时间的格式，需要和partition.timestamp-pattern配合使用。假设dt字段的值是2023-10-18，那么formatter则应该是yyyy-MM-dd，用于将dt字段的值解析成时间 -->
    <property>
        <name>partition.timestamp-formatter</name>
        <value>yyyy-MM-dd</value>
        <description>
            The formatter to format timestamp from string. It can be used with 'partition.timestamp-pattern' to create a formatter using the specified value.
                Default formatter is 'yyyy-MM-dd HH:mm:ss' and 'yyyy-MM-dd'.
                Supports multiple partition fields like '$year-$month-$day $hour:00:00'.
                The timestamp-formatter is compatible with Java's DateTimeFormatter.
        </description>
        <default>(none)</default>
    </property>

    <!-- 将表中哪个字段作为分区时间解析的字段。假设表的分区字段是year,month,day这三个字段，那么该配置就可以配置为$year-$month-$day，然后partition.timestamp-formatter配置成yyyy-MM-dd -->
    <property>
        <name>partition.timestamp-pattern</name>
        <value>$dt</value>
        <description>
            You can specify a pattern to get a timestamp from partitions. The formatter pattern is defined by 'partition.timestamp-formatter'.
                By default, read from the first field.
                If the timestamp in the partition is a single field called 'dt', you can use '$dt'.
                If it is spread across multiple fields for year, month, day, and hour, you can use '$year-$month-$day $hour:00:00'.
                If the timestamp is in fields dt and hour, you can use '$dt $hour:00:00'.
        </description>
        <default>(none)</default>
    </property>
    <!-- partition过期配置:end  -->
</configuration>




























