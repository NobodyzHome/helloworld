<configuration>
    <!-- 指定上传文件时，每个切块的大小 -->
    <property>
        <!-- The default block size for new files, in bytes. You can use the following suffix (case insensitive): k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), Or provide complete size in bytes (such as 134217728 for 128 MB). -->
        <name>dfs.blocksize</name>
        <!-- default: 128MB -->
        <value>128m</value>
    </property>

    <!-- 指定上传文件时，每个切块的默认副本数量 -->
    <property>
        <!-- Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. -->
        <name>dfs.replication</name>
        <!-- default: 3-->
        <value>2</value>
    </property>

    <!-- 指定上传文件时，每个切换的最大副本数量 -->
    <property>
        <!-- Maximal block replication. -->
        <name>dfs.replication.max</name>
        <!-- default: 512 -->
        <value>512</value>
    </property>

    <!-- 指定上传文件时，每个切换的最小副本数量 -->
    <property>
        <!-- Minimal block replication. -->
        <name>dfs.namenode.replication.min</name>
        <!-- default: 1 -->
        <value>1</value>
    </property>

    <!-- 如果是在windows系统下想连接hdfs，需要启动该配置 -->
    <property>
        <!-- Specifies whether to flush edit log file channel. When set, expensive FileChannel#force calls are skipped and synchronous disk writes are enabled instead by opening the edit log file with RandomAccessFile("rws") flags. This can significantly improve the performance of edit log writes on the Windows platform. Note that the behavior of the "rws" flags is platform and hardware specific and might not provide the same level of guarantees as FileChannel#force. For example, the write will skip the disk-cache on SAS and SCSI devices while it might not on SATA devices. This is an expert level setting, change with caution. -->
        <name>dfs.namenode.edits.noeditlogchannelflush</name>
        <!-- default: false -->
        <value>false</value>
    </property>

    <!-- 设置hdfs中namenode把数据存储在本地文件系统的哪个目录（默认配置中，参数hadoop.tmp.dir是在core-site中配置的） -->
    <property>
        <!-- Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy. -->
        <name>dfs.namenode.name.dir</name>
        <!-- default: file://${hadoop.tmp.dir}/dfs/name	 -->
        <value>file://${hadoop.tmp.dir}/dfs/name</value>
    </property>

    <!-- 设置hdfs中datanode把数据存储在本地文件系统的哪个目录（默认配置中，参数hadoop.tmp.dir是在core-site中配置的） -->
    <property>
        <!-- Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. The directories should be tagged with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS storage policies. The default storage type will be DISK if the directory does not have a storage type tagged explicitly. Directories that do not exist will be created if local filesystem permission allows. -->
        <name>dfs.datanode.data.dir</name>
        <!-- default: file://${hadoop.tmp.dir}/dfs/data -->
        <value>file://${hadoop.tmp.dir}/dfs/data</value>
    </property>

    <!-- 设置上传文件时，切块大小最少是多少，避免向hdfs上传非常多微小（tiny）的切块（例如上传一个1TB的文件，以1mb一个切块进行切块，就会产生很多微小的切块）。大量对hdfs进行微小切块的读和写会降低hdfs的性能。 -->
    <property>
        <!-- Minimum block size in bytes, enforced by the Namenode at create time. This prevents the accidental creation of files with tiny block sizes (and thus many blocks), which can degrade performance. Support multiple size unit suffix(case insensitive), as described in dfs.blocksize. -->
        <name>dfs.namenode.fs-limits.min-block-size</name>
        <!-- default: 1048576 -->
        <value>1k</value>
    </property>

    <!-- 设置上传文件时，一个文件最多产生的切块数量，避免产生大量的切块，降低hdfs的读写性能 -->
    <property>
        <!-- Maximum number of blocks per file, enforced by the Namenode on write. This prevents the creation of extremely large files which can degrade performance. -->
        <name>dfs.namenode.fs-limits.max-blocks-per-file</name>
        <!-- default：10000 -->
        <value>100</value>
    </property>

    <!-- 设置namenode在告诉客户端连接datanode的地址时，是否要把datanode的hostname告诉给客户端。如果为false的话，namenode可能直接把datanode的ip地址告诉客户端，而不是host。 -->
    <property>
        <!-- Whether clients should use datanode hostnames when connecting to datanodes. -->
        <name>dfs.client.use.datanode.hostname</name>
        <!-- default: false -->
        <value>true</value>
    </property>
</configuration>