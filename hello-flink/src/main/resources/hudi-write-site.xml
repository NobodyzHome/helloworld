'connector' = 'hudi',
'path' = 'hdfs://ns14/user/mart_coo_dev/tmp.db/hudi_busi_waybill_info_test2',
'table.type' = 'COPY_ON_WRITE',
'write.operation' = 'insert',
-- write subTask中单个bucket内存缓冲使用量，如果超过了这个配置，就对这个bucket进行flush操作，从内存中刷写到磁盘
'write.batch.size' = '256',
-- write subTask中所有bucket的最大内存缓冲使用量，如果超过了这个配置，就会把所有bucket中内存缓存使用量最大的那个bucket进行flush操作，从内存中刷写到磁盘
'write.task.max.size' = '1024',
'write.log_block.size' = '128',
'write.log.max.size' = '2048',
'write.parquet.block.size' = '120',
'write.parquet.max.file.size' = '120',
'write.precombine' = 'true',
'precombine.field' = 'ts',
'payload.class' = 'org.apache.hudi.common.model.EventTimeAvroPayload',
'hoodie.payload.ordering.field' = 'ts',
'record.merger.impls' = 'org.apache.hudi.common.model.HoodieAvroRecordMerger',
'index.state.ttl' = '0',
'index.global.enabled' = 'true',
'index.bootstrap.enabled' = 'true',
'compaction.schedule.enabled' = 'true',
'compaction.async.enabled' = 'true',
'compaction.trigger.strategy' = 'num_commits',
'compaction.delta_commits' = '3',
'compaction.delta_seconds' = '86400',
'compaction.timeout.seconds' = '1200',
'compaction.max_memory' = '300',
'clean.policy' = 'KEEP_LATEST_FILE_VERSIONS',
'clean.retain_file_versions' = '3',
'hoodie.clean.max.commits' = '1'